# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# + [markdown] id="gEJNiFMuxa6x"
# # ê°€ì¹˜ ë°˜ë³µ

# + id="dL09IfYlxAz3" executionInfo={"status": "ok", "timestamp": 1650110069423, "user_tz": -540, "elapsed": 488, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}}
import numpy as np
import time
import copy


# + [markdown] id="_QybMndKxW52"
# ## ê·¸ë¦¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜

# + id="8kriUlj0xWee" executionInfo={"status": "ok", "timestamp": 1650110069925, "user_tz": -540, "elapsed": 8, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}}
# V table ê·¸ë¦¬ê¸°    
def show_v_table(v_table, env):    
    for i in range(env.reward.shape[0]):        
        print("+-----------------"*env.reward.shape[1],end="")
        print("+")
        for k in range(3):
            print("|",end="")
            for j in range(env.reward.shape[1]):
                if k==0:
                    print("                 |",end="")
                if k==1:
                        print("   {0:8.2f}      |".format(v_table[i,j]),end="")
                if k==2:
                    print("                 |",end="")
            print()
    print("+-----------------"*env.reward.shape[1],end="")
    print("+")

# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°
def show_policy(policy,env):
    for i in range(env.reward.shape[0]):        
        print("+-----------------"*env.reward.shape[1],end="")
        print("+")
        for k in range(3):
            print("|",end="")
            for j in range(env.reward.shape[1]):
                if k==0:
                    print("                 |",end="")
                if k==1:
                    if policy[i,j] == 0:
                        print("      â†‘         |",end="")
                    elif policy[i,j] == 1:
                        print("      â†’         |",end="")
                    elif policy[i,j] == 2:
                        print("      â†“         |",end="")
                    elif policy[i,j] == 3:
                        print("      â†         |",end="")
                if k==2:
                    print("                 |",end="")
            print()
    print("+-----------------"*env.reward.shape[1],end="")
    print("+")


# + [markdown] id="lgsQsky3w9nQ"
# ## Environment êµ¬í˜„

# + id="j-5_IGGDw6dI" executionInfo={"status": "ok", "timestamp": 1650110069925, "user_tz": -540, "elapsed": 7, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}}
class Environment():
    
    # 1. ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •
    cliff = -3
    road = -1
    goal = 1
    
    # 2. ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •
    goal_position = [2,2]
    
    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì
    reward_list = [[road,road,road],
                   [road,road,road],
                   [road,road,goal]]
    
    # 4. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì
    reward_list1 = [["road","road","road"],
                    ["road","road","road"],
                    ["road","road","goal"]]
    
    # 5. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •
    def __init__(self):
        self.reward = np.asarray(self.reward_list)    

    # 6. ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)
    def move(self, agent, action):
        
        done = False
        
        # 6.1 í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°
        new_pos = agent.pos + agent.action[action]
        
        # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸
        if self.reward_list1[agent.pos[0]][agent.pos[1]] == "goal":
            reward = self.goal
            observation = agent.set_pos(agent.pos)
            done = True
        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸    
        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:
            reward = self.cliff
            observation = agent.set_pos(agent.pos)
            done = True
        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´
        else:
            observation = agent.set_pos(new_pos)
            reward = self.reward[observation[0],observation[1]]
            
        return observation, reward, done


# + [markdown] id="tT7seyk7xIBs"
# ## Agent êµ¬í˜„

# + id="XK7_RCYNxJyr" executionInfo={"status": "ok", "timestamp": 1650110069926, "user_tz": -540, "elapsed": 7, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}}
class Agent():
    
    # 1. í–‰ë™ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ì˜ ì¢Œí‘œ ì´ë™(ìœ„, ì˜¤ë¥¸ìª½, ì•„ë˜, ì™¼ìª½) 
    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])
    
    # 2. ê° í–‰ë™ë³„ ì„ íƒí™•ë¥ 
    select_action_pr = np.array([0.25,0.25,0.25,0.25])
    
    # 3. ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ìœ„ì¹˜ ì €ì¥
    def __init__(self):
        self.pos = (0,0)
    
    # 4. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì €ì¥
    def set_pos(self,position):
        self.pos = position
        return self.pos
    
    # 5. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
    def get_pos(self):
        return self.pos


# + [markdown] id="-RrVAIaox27F"
# ## ê°€ì¹˜ ë°˜ë³µ í•¨ìˆ˜

# + id="OopZxTb4wSTK" executionInfo={"status": "ok", "timestamp": 1650110069926, "user_tz": -540, "elapsed": 7, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}}
def finding_optimal_value_function(env, agent, v_table):
    k = 1
    gamma = 0.9
    while(True):
        # Î”â†0
        delta=0
        #  vâ†ğ‘‰(ğ‘ )
        temp_v = copy.deepcopy(v_table)

        # ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ :
        for i in range(env.reward.shape[0]):
            for j in range(env.reward.shape[1]):
                temp = -1e+10
#                 print("s({0}):".format(i*env.reward.shape[0]+j))
                # ğ‘‰(ğ‘ )â† max(a)â¡âˆ‘ğ‘ƒ(ğ‘ '|ğ‘ ,ğ‘)[ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ ') +ğ›¾ğ‘‰(ğ‘ ')]
                # ê°€ëŠ¥í•œ í–‰ë™ì„ ì„ íƒ
                for action in range(len(agent.action)):
                    agent.set_pos([i,j])
                    observation, reward, done = env.move(agent, action)
#                     print("{0:.2f} = {1:.2f} + {2:.2f} * {3:.2f}" .format(reward + gamma* v_table[observation[0],observation[1]],reward, gamma,v_table[observation[0],observation[1]]))
                    #ì´ë™í•œ ìƒíƒœì˜ ê°€ì¹˜ê°€ tempë³´ë‹¤ í¬ë©´
                    if temp < reward + gamma*v_table[observation[0],observation[1]]:
                        # temp ì— ìƒˆë¡œìš´ ê°€ì¹˜ë¥¼ ì €ì¥
                        temp = reward + gamma*v_table[observation[0],observation[1]]  
#                 print("V({0}) :max = {1:.2f}".format(i*env.reward.shape[0]+j,temp))
#                 print()
                # ì´ë™ ê°€ëŠ¥í•œ ìƒíƒœ ì¤‘ ê°€ì¥ í° ê°€ì¹˜ë¥¼ ì €ì¥
                v_table[i,j] = temp

        #  âˆ†â†maxâ¡(âˆ†,|vâˆ’ğ‘‰(ğ‘ )|)
        # ì´ì „ ê°€ì¹˜ì™€ ë¹„êµí•´ì„œ í° ê°’ì„ deltaì— ì €ì¥
        # ê³„ì‚°ì „ê³¼ ê³„ì‚°í›„ì˜ ê°€ì¹˜ì˜ ì°¨ì´ ê³„ì‚°
        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  
        # 7. âˆ† <ğœƒê°€ ì‘ì€ ì–‘ìˆ˜ ì¼ ë•Œê¹Œì§€ ë°˜ë³µ
        if delta < 0.0000001:
            break
            
#         if k < 4 or k > 150:
#             print("V{0}(S) : k = {1:3d}    delta = {2:0.6f}".format(k,k, delta))
#             show_v_table(np.round(v_table,2),env)
        print("V{0}(S) : k = {1:3d}    delta = {2:0.6f}".format(k,k, delta))
        show_v_table(np.round(v_table,2),env)
        k +=1
        
    return v_table

def policy_extraction(env, agent, v_table, optimal_policy):

    gamma = 0.9
    
    #ì •ì±… ğœ‹ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ì¶œ
    # ğœ‹(ğ‘ )â† argmax(a)â¡âˆ‘ğ‘ƒ(ğ‘ '|ğ‘ ,ğ‘)[ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ ') +ğ›¾ğ‘‰(ğ‘ ')]
    # ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ : 
    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            temp =  -1e+10
            # ê°€ëŠ¥í•œ í–‰ë™ì¤‘ ê°€ì¹˜ê°€ ê°€ì¥ë†’ì€ ê°’ì„ policy[i,j]ì— ì €ì¥
            for action in range(len(agent.action)):
                agent.set_pos([i,j])
                observation, reward, done = env.move(agent,action)
                if temp < reward + gamma * v_table[observation[0],observation[1]]:
                    optimal_policy[i,j] = action
                    temp = reward + gamma * v_table[observation[0],observation[1]]
                
    return optimal_policy



# + colab={"base_uri": "https://localhost:8080/"} id="Y90PiuNxwwWm" executionInfo={"status": "ok", "timestamp": 1650110070968, "user_tz": -540, "elapsed": 1048, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}} outputId="789297e4-32e6-4070-aade-8238e4c23036"
# ê°€ì¹˜ ë°˜ë³µ

# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”
np.random.seed(0)
env = Environment()
agent = Agent()

# ì´ˆê¸°í™”
# ëª¨ë“  ğ‘ âˆˆğ‘†^+ì— ëŒ€í•´ ğ‘‰(ğ‘ )âˆˆğ‘…ì„ ì„ì˜ë¡œ ì„¤ì •
v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])

print("Initial random V0(S)")
show_v_table(np.round(v_table,2),env)
print()

optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))

print("start Value iteration")
print()

# ì‹œì‘ ì‹œê°„ ë³€ìˆ˜ì— ì €ì¥
start_time = time.time()

v_table = finding_optimal_value_function(env, agent, v_table)

optimal_policy = policy_extraction(env, agent, v_table, optimal_policy)

                
print("total_time = {}".format(np.round(time.time()-start_time),2))
print()
print("Optimal policy")
show_policy(optimal_policy, env)
