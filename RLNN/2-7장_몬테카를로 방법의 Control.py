# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# + [markdown] id="gEJNiFMuxa6x"
# # ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²•ì˜ Control

# + executionInfo={"elapsed": 359, "status": "ok", "timestamp": 1651296571271, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="dL09IfYlxAz3"
import numpy as np
from tqdm import tqdm


# + [markdown] id="_QybMndKxW52"
# ## ê·¸ë¦¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜

# + executionInfo={"elapsed": 12, "status": "ok", "timestamp": 1651296571969, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="8kriUlj0xWee"
# Q table ê·¸ë¦¬ê¸°
def show_q_table(q_table,env):
    for i in range(env.reward.shape[0]):
        print("+-----------------"*env.reward.shape[1],end="")
        print("+")
        for k in range(3):
            print("|",end="")
            for j in range(env.reward.shape[1]):
                if k==0:
                    print("{0:10.2f}       |".format(q_table[i,j,0]),end="")
                if k==1:
                    print("{0:6.2f}    {1:6.2f} |".format(q_table[i,j,3],q_table[i,j,1]),end="")
                if k==2:
                    print("{0:10.2f}       |".format(q_table[i,j,2]),end="")
            print()
    print("+-----------------"*env.reward.shape[1],end="")
    print("+")

# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°
def show_policy(policy,env):
    for i in range(env.reward.shape[0]):        
        print("+-----------------"*env.reward.shape[1],end="")
        print("+")
        for k in range(3):
            print("|",end="")
            for j in range(env.reward.shape[1]):
                if k==0:
                    print("                 |",end="")
                if k==1:
                    if policy[i,j] == 0:
                        print("      â†‘         |",end="")
                    elif policy[i,j] == 1:
                        print("      â†’         |",end="")
                    elif policy[i,j] == 2:
                        print("      â†“         |",end="")
                    elif policy[i,j] == 3:
                        print("      â†         |",end="")
                if k==2:
                    print("                 |",end="")
            print()
    print("+-----------------"*env.reward.shape[1],end="")
    print("+")


# + [markdown] id="lgsQsky3w9nQ"
# ## Environment êµ¬í˜„

# + executionInfo={"elapsed": 12, "status": "ok", "timestamp": 1651296571970, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="j-5_IGGDw6dI"
class Environment():
    
    # 1. ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •
    cliff = -3
    road = -1
    goal = 1
    
    # 2. ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •
    goal_position = [2,2]
    
    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì
    reward_list = [[road,road,road],
                   [road,road,road],
                   [road,road,goal]]
    
    # 4. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì
    reward_list1 = [["road","road","road"],
                    ["road","road","road"],
                    ["road","road","goal"]]
    
    # 5. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •
    def __init__(self):
        self.reward = np.asarray(self.reward_list)    

    # 6. ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)
    def move(self, agent, action):
        
        done = False
        
        # 6.1 í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°
        new_pos = agent.pos + agent.action[action]
        
        # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸
        if self.reward_list1[agent.pos[0]][agent.pos[1]] == "goal":
            reward = self.goal
            observation = agent.set_pos(agent.pos)
            done = True
        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸    
        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:
            reward = self.cliff
            observation = agent.set_pos(agent.pos)
            done = True
        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´
        else:
            observation = agent.set_pos(new_pos)
            reward = self.reward[observation[0],observation[1]]
            
        return observation, reward, done


# + [markdown] id="tT7seyk7xIBs"
# ## Agent êµ¬í˜„

# + executionInfo={"elapsed": 11, "status": "ok", "timestamp": 1651296571970, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="XK7_RCYNxJyr"
class Agent():
    
    # 1. í–‰ë™ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ì˜ ì¢Œí‘œ ì´ë™(ìœ„, ì˜¤ë¥¸ìª½, ì•„ë˜, ì™¼ìª½) 
    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])
    
    # 2. ê° í–‰ë™ë³„ ì„ íƒí™•ë¥ 
    select_action_pr = np.array([0.25,0.25,0.25,0.25])
    
    # 3. ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ìœ„ì¹˜ ì €ì¥
    def __init__(self):
        self.pos = (0,0)
    
    # 4. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì €ì¥
    def set_pos(self,position):
        self.pos = position
        return self.pos
    
    # 5. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
    def get_pos(self):
        return self.pos


# + [markdown] id="-RrVAIaox27F"
# ## ì—í”¼ì†Œë“œ ìƒì„± í•¨ìˆ˜

# + executionInfo={"elapsed": 11, "status": "ok", "timestamp": 1651296571971, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="OopZxTb4wSTK"
def generate_episode_with_policy(env, agent, first_visit, policy):
    gamma = 0.09
    # ì—í”¼ì†Œë“œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    episode = []
    # ì´ì „ì— ë°©ë¬¸ì—¬ë¶€ ì²´í¬
    visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))
    
    # ì—ì´ì „íŠ¸ëŠ” í•­ìƒ (0,0)ì—ì„œ ì¶œë°œ
    i = 0
    j = 0
    agent.set_pos([i,j])    
    #ì—í”¼ì†Œë“œì˜ ìˆ˜ìµì„ ì´ˆê¸°í™”
    G = 0
    #ê°ì‡„ìœ¨ì˜ ì§€ìˆ˜
    step = 0
    max_step = 100
    # ì—í”¼ì†Œë“œ ìƒì„±
    for k in range(max_step):
        pos = agent.get_pos()        
        # í˜„ì¬ ìƒíƒœì˜ ì •ì±…ì„ ì´ìš©í•´ í–‰ë™ì„ ì„ íƒí•œ í›„ ì´ë™
        action = np.random.choice(range(0,len(agent.action)), p=policy[pos[0],pos[1],:]) 
        observaetion, reward, done = env.move(agent, action)    
        
        if first_visit:
            # ì—í”¼ì†Œë“œì— ì²« ë°©ë¬¸í•œ ìƒíƒœì¸ì§€ ê²€ì‚¬ :
            # visit[pos[0],pos[1]] == 0 : ì²« ë°©ë¬¸
            # visit[pos[0],pos[1]] == 1 : ì¤‘ë³µ ë°©ë¬¸
            if visit[pos[0],pos[1],action] == 0:   
                # ì—í”¼ì†Œë“œê°€ ëë‚ ë•Œê¹Œì§€ Gë¥¼ ê³„ì‚°
                G += gamma**(step) * reward        
                # ë°©ë¬¸ ì´ë ¥ í‘œì‹œ
                visit[pos[0],pos[1],action] = 1
                step += 1               
                # ë°©ë¬¸ ì´ë ¥ ì €ì¥(ìƒíƒœ, í–‰ë™, ë³´ìƒ)
                episode.append((pos,action, reward))
        else:
            G += gamma**(step) * reward
            step += 1                   
            episode.append((pos,action,reward))            

        # ì—í”¼ì†Œë“œê°€ ì¢…ë£Œí–ˆë‹¤ë©´ ë£¨í”„ì—ì„œ íƒˆì¶œ
        if done == True:                
            break        
            
    return i, j, G, episode


# + [markdown] id="gQtwLOM2qpLZ"
# ## Ïµ-ì •ì±…ì„ ì´ìš©í•˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ control ì•Œê³ ë¦¬ì¦˜

# + colab={"base_uri": "https://localhost:8080/"} executionInfo={"elapsed": 3162, "status": "ok", "timestamp": 1651296575122, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="Y90PiuNxwwWm" outputId="ed7d6d9f-4a10-4db4-8d0f-ed4f7dee3f8e"
np.random.seed(0)
# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”
env = Environment()
agent = Agent()

# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:
# # ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’ (í–‰ë™ ê°œìˆ˜, ë¯¸ë¡œ ì„¸ë¡œ, ë¯¸ë¡œ ê°€ë¡œ)
Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))
print("Initial Q(s,a)")
show_q_table(Q_table,env)

# ìƒíƒœë¥¼ ë°©ë¬¸í•œ íšŸìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”
Q_visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))

# ë¯¸ë¡œ ëª¨ë“  ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ì €ì¥í•˜ëŠ” í…Œì´ë¸”
# ê° ìƒíƒœì—ì„œ Q ê°’ì´ ê°€ì¥ í° í–‰ë™ì„ ì„ íƒ í›„ optimal_a ì— ì €ì¥
optimal_a = np.zeros((env.reward.shape[0],env.reward.shape[1]))
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        optimal_a[i,j] = np.argmax(Q_table[i,j,:])
print("initial optimal_a")
show_policy(optimal_a,env)

# Ï€(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ğœ–âˆ’íƒìš• ì •ì±…
# ë¬´ì‘ìœ„ë¡œ í–‰ë™ì„ ì„ íƒí•˜ë„ë¡ ì§€ì •
policy = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))

# í•œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ê³„ì‚°
epsilon = 0.8
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        for k in range(len(agent.action)):
            if optimal_a[i,j] == k:
                policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)
            else:
                policy[i,j,k] = epsilon/len(agent.action)
print("Initial Policy")
show_q_table(policy,env)


# ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ ê¸¸ì´ë¥¼ ì§€ì •
max_episode = 10000

# first visit ë¥¼ ì‚¬ìš©í• ì§€ every visitë¥¼ ì‚¬ìš©í•  ì§€ ê²°ì •
# first_visit = True : first visit
# first_visit = False : every visit
first_visit = True
if first_visit:
    print("start first visit MC")
else : 
    print("start every visit MC")
print()

gamma = 0.09
for epi in tqdm(range(max_episode)):
# for epi in range(max_episode):

    # Ï€ë¥¼ ì´ìš©í•´ì„œ ì—í”¼ì†Œë“œ 1ê°œë¥¼ ìƒì„±
    x,y,G,episode = generate_episode_with_policy(env, agent, first_visit, policy)
    
    for step_num in range(len(episode)):
        G = 0
        # episode[step_num][0][0] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ x ì¢Œí‘œ
        # episode[step_num][0][1] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ y ì¢Œí‘œ
        # episode[step_num][1] : step_numë²ˆì§¸ ìƒíƒœì—ì„œ ì„ íƒí•œ í–‰ë™
        i = episode[step_num][0][0]
        j = episode[step_num][0][1]
        action = episode[step_num][1]
        
        # ì—í”¼ì†Œë“œ ì‹œì‘ì ì„ ì¹´ìš´íŠ¸
        Q_visit[i,j,action] += 1

        # ì„œë¸Œ ì—í”¼ì†Œë“œ (episode[step_num:])ì˜ ì¶œë°œë¶€í„° ëê¹Œì§€ ìˆ˜ìµ Gë¥¼ ê³„ì‚°
        # k[2] : episode[step_num][2] ê³¼ ê°™ìœ¼ë©° step_num ë²ˆì§¸ ë°›ì€ ë³´ìƒ
        # step : ê°ì‡„ìœ¨
        for step, k in enumerate(episode[step_num:]):
            G += gamma**(step)*k[2]

        # Incremental mean : ğ‘„(ğ‘ ,ğ‘)â†ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’(ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›(ğ‘ ,ğ‘)) 
        Q_table[i,j,action] += 1 / Q_visit[i,j,action]*(G-Q_table[i,j,action])
    
    # (c) ì—í”¼ì†Œë“œ ì•ˆì˜ ê° sì— ëŒ€í•´ì„œ :
    # ë¯¸ë¡œ ëª¨ë“  ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ì €ì¥í•  ê³µê°„ ë§ˆë ¨
    # ğ‘âˆ— â†argmax_a ğ‘„(ğ‘ ,ğ‘)
    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            optimal_a[i,j] = np.argmax(Q_table[i,j,:])            
   
    # ëª¨ë“  ğ‘âˆˆğ´(ğ‘†) ì— ëŒ€í•´ì„œ :
    # ìƒˆë¡œ ê³„ì‚°ëœ optimal_a ë¥¼ ì´ìš©í•´ì„œ í–‰ë™ ì„ íƒ í™•ë¥  policy (Ï€) ê°±ì‹ 
    epsilon = 1 - epi/max_episode

    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            for k in range(len(agent.action)):
                if optimal_a[i,j] == k:
                    policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)
                else:
                    policy[i,j,k] = epsilon/len(agent.action)

print("Final Q(s,a)")
show_q_table(Q_table,env)
print("Final policy")
show_q_table(policy,env)
print("Final optimal_a")
show_policy(optimal_a,env)
