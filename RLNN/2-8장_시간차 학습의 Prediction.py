# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# + [markdown] id="5agojReWIYlV"
# # ì‹œê°„ì°¨ í•™ìŠµì˜ Prediction

# + executionInfo={"elapsed": 337, "status": "ok", "timestamp": 1651652285485, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="CwtfaBKOhqaB"
import numpy as np
from tqdm import tqdm


# + [markdown] id="JQkVjrbvI2_s"
# ## ê·¸ë¦¼ê·¸ë¦¬ëŠ” í•¨ìˆ˜

# + executionInfo={"elapsed": 4, "status": "ok", "timestamp": 1651652285825, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="-maf_uhdI2k9"
# V table ê·¸ë¦¬ê¸°    
def show_v_table(v_table, env):    
    for i in range(env.reward.shape[0]):        
        print("+-----------------"*env.reward.shape[1],end="")
        print("+")
        for k in range(3):
            print("|",end="")
            for j in range(env.reward.shape[1]):
                if k==0:
                    print("                 |",end="")
                if k==1:
                        print("   {0:8.2f}      |".format(v_table[i,j]),end="")
                if k==2:
                    print("                 |",end="")
            print()
    print("+-----------------"*env.reward.shape[1],end="")
    print("+")


# + [markdown] id="8EIdUJwmIgFS"
# ## Environment êµ¬í˜„

# + executionInfo={"elapsed": 4, "status": "ok", "timestamp": 1651652285825, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="HytMoTUaeV4-"
class Environment():
    
    # ë³´ìƒ ì„¤ì •
    cliff = -3
    road = -1
    goal = 1

    reward_list = [[road,road,road],
                   [road,road,road],
                   [road,road,goal]]

    reward_list1 = [["road","road","road"],
                    ["road","road","road"],
                    ["road","road","goal"]]

    def __init__(self):
        self.reward = np.array(self.reward_list)
     
    def move(self, agent, action):
        
        done = False

        new_pos = agent.pos + agent.action[action]

         # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸
        if self.reward_list1[agent.pos[0]][agent.pos[1]] == "goal":
            reward = self.goal
            observation = agent.set_pos(agent.pos)
            done = True
        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸    
        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:
            reward = self.cliff
            observation = agent.set_pos(agent.pos)
            done = True
        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´
        else:
            observation = agent.set_pos(new_pos)
            reward = self.reward[observation[0],observation[1]]
            
        return observation, reward, done
      


# + [markdown] id="eNm5b3QNIhjx"
# ## Agent êµ¬í˜„

# + executionInfo={"elapsed": 9, "status": "ok", "timestamp": 1651652286252, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="KApr6k3LiJmF"
class Agent():

    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])
    
    select_action_pr = np.array([0.25,0.25,0.25,0.25])

    def __init__(self):
        self.pos = (0,0)
    
    def set_pos(self,position):
        self.pos = position
        return self.pos

    def get_pos(self):
        return self.pos


# + colab={"base_uri": "https://localhost:8080/"} executionInfo={"elapsed": 366, "status": "ok", "timestamp": 1651652286610, "user": {"displayName": "Jungi Kim", "userId": "13599710065611566056"}, "user_tz": -540} id="tBxFRf1JIUHU" outputId="90b10be8-a688-45a3-b203-febac49c0680"
# TD(0) prediction
np.random.seed(0)
# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”
env = Environment()
agent = Agent()
gamma = 0.9

#ì´ˆê¸°í™” : 
#Ï€â† í‰ê°€í•  ì •ì±…
# ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ë„ë¡ ì§€ì •
#ğ‘‰â† ì„ì˜ì˜ ìƒíƒœê°€ì¹˜ í•¨ìˆ˜
V = np.zeros((env.reward.shape[0], env.reward.shape[1]))

# ìµœëŒ€ ì—í”¼ì†Œë“œ, ì—í”¼ì†Œë“œì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •
max_episode = 10000
max_step = 100

alpha = 0.01

print("start TD(0) prediction")

# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :
for epi in tqdm(range(max_episode)):
    delta =0
    # s ë¥¼ ì´ˆê¸°í™”
    i = 0
    j = 0
    agent.set_pos([i,j])

    #  ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :
    for k in range(max_step):
        pos = agent.get_pos()
        # aâ†ìƒíƒœ ğ‘  ì—ì„œ ì •ì±… Ï€ì— ì˜í•´ ê²°ì •ëœ í–‰ë™ 
        # ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ê²Œ í•¨
        action = np.random.randint(0,4)
        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìˆ˜ rê³¼ ë‹¤ìŒ ìƒíƒœ sâ€™ë¥¼ ê´€ì¸¡
        # sâ†ğ‘ '
        observation, reward, done = env.move(agent,action)
        # V(ğ‘ )â†V(ğ‘ )+ Î±[ğ‘Ÿ+ğ›¾ğ‘‰(ğ‘ ^)âˆ’ğ‘‰(ğ‘ )]
        V[pos[0],pos[1]] += alpha * (reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]])
        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ
        if done == True:
            break
            
print("V(s)")
show_v_table(np.round(V,2),env)
