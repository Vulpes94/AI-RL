{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2장_행동가치함수.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPMN8voydlcNVAXPRtbz8hy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#행동 가치 함수"],"metadata":{"id":"h794qK3Oic0h"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"cIrr-8sBi8sj","executionInfo":{"status":"ok","timestamp":1649310192600,"user_tz":-540,"elapsed":517,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["##그림그리는 함수"],"metadata":{"id":"wLW0z-vOiiBh"}},{"cell_type":"code","source":["# Q table 그리기\n","def show_q_table(q_table,env):\n","    for i in range(env.reward.shape[0]):\n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n","                if k==1:\n","                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n","                if k==2:\n","                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")\n","\n","# 정책 policy 화살표로 그리기\n","def show_q_table_arrow(q_table,env):\n","    for i in range(env.reward.shape[0]):        \n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    if np.max(q[i,j,:]) == q[i,j,0]:\n","                        print(\"        ↑       |\",end=\"\")\n","                    else:\n","                        print(\"                 |\",end=\"\")\n","                if k==1:                    \n","                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n","                        print(\"      ←  →     |\",end=\"\")\n","                    elif np.max(q[i,j,:]) == q[i,j,1]:\n","                        print(\"          →     |\",end=\"\")\n","                    elif np.max(q[i,j,:]) == q[i,j,3]:\n","                        print(\"      ←         |\",end=\"\")\n","                    else:\n","                        print(\"                 |\",end=\"\")\n","                if k==2:\n","                    if np.max(q[i,j,:]) == q[i,j,2]:\n","                        print(\"        ↓       |\",end=\"\")\n","                    else:\n","                        print(\"                 |\",end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")    "],"metadata":{"id":"K0mDS_iAjP7D","executionInfo":{"status":"ok","timestamp":1649310289944,"user_tz":-540,"elapsed":371,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["##Environment 구현"],"metadata":{"id":"hVxMbO0_ip7c"}},{"cell_type":"code","source":["class Environment():\n","    \n","    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n","    cliff = -3\n","    road = -1\n","    goal = 1\n","    \n","    # 2. 목적지 좌표 설정\n","    goal_position = [2,2]\n","    \n","    # 3. 보상 리스트 숫자\n","    reward_list = [[road,road,road],\n","                   [road,road,road],\n","                   [road,road,goal]]\n","    \n","    # 4. 보상 리스트 문자\n","    reward_list1 = [[\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"goal\"]]\n","    \n","    # 5. 보상 리스트를 array로 설정\n","    def __init__(self):\n","        self.reward = np.asarray(self.reward_list)    \n","\n","    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n","    def move(self, agent, action):\n","        \n","        done = False\n","        \n","        # 6.1 행동에 따른 좌표 구하기\n","        new_pos = agent.pos + agent.action[action]\n","        \n","        # 6.2 현재좌표가 목적지 인지확인\n","        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n","            reward = self.goal\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.3 이동 후 좌표가 미로 밖인 확인    \n","        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n","            reward = self.cliff\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.4 이동 후 좌표가 길이라면\n","        else:\n","            observation = agent.set_pos(new_pos)\n","            reward = self.reward[observation[0],observation[1]]\n","            \n","        return observation, reward, done"],"metadata":{"id":"hNHgmsNjiM1T","executionInfo":{"status":"ok","timestamp":1649310198114,"user_tz":-540,"elapsed":366,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##Agent 구현"],"metadata":{"id":"odBKf64tikWQ"}},{"cell_type":"code","source":["class Agent():\n","    \n","    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n","    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n","    \n","    # 2. 각 행동별 선택확률\n","    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n","    \n","    # 3. 에이전트의 초기 위치 저장\n","    def __init__(self):\n","        self.pos = (0,0)\n","    \n","    # 4. 에이전트의 위치 저장\n","    def set_pos(self,position):\n","        self.pos = position\n","        return self.pos\n","    \n","    # 5. 에이전트의 위치 불러오기\n","    def get_pos(self):\n","        return self.pos"],"metadata":{"id":"WOQE9pHPiWpR","executionInfo":{"status":"ok","timestamp":1649310201579,"user_tz":-540,"elapsed":358,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["##행동 가치 함수"],"metadata":{"id":"pGYH8lvAh0jf"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"LnidkLh8hSrr","executionInfo":{"status":"ok","timestamp":1649310204526,"user_tz":-540,"elapsed":521,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"outputs":[],"source":["# 재귀적으로 행동가치함수를 계산하는 함수\n","\n","# 행동 가치 함수\n","def action_value_function(env, agent, act, G, max_step, now_step):   \n","    \n","    # 1. 감가율 설정\n","    gamma = 0.9\n","    \n","    # 2. 현재 위치가 목적지인지 확인\n","    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n","        return env.goal\n","\n","    # 3. 마지막 상태는 보상만 계산\n","    if (max_step == now_step):\n","        observation, reward, done = env.move(agent, act)\n","        G += agent.select_action_pr[act]*reward\n","        return G\n","    \n","    # 4. 현재 상태의 보상을 계산한 후 다음 행동과 함께 다음 step으로 이동\n","    else:\n","        # 4.1현재 위치 저장\n","        pos1 = agent.get_pos()\n","        observation, reward, done = env.move(agent, act)\n","        G += agent.select_action_pr[act] * reward\n","        \n","        # 4.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n","        if done == True:            \n","            if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n","                agent.set_pos(pos1)\n","            \n","        # 4.3 현재 위치를 다시 저장\n","        pos1 = agent.get_pos()\n","        \n","        # 4.4 현재 위치에서 가능한 모든 행동을 선택한 후 이동\n","        for i in range(len(agent.action)):\n","            agent.set_pos(pos1)\n","            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n","            G += agent.select_action_pr[i] * gamma * next_v\n","        return G"]},{"cell_type":"markdown","source":["미로의 각 상태의 행동가치함수를 구하는 함수"],"metadata":{"id":"4Oz-zwfZh72X"}},{"cell_type":"code","source":["# 재귀적으로 행동의 가치를 계산\n","\n","# 1. 환경 초기화\n","env = Environment()\n","\n","# 2. 에이전트 초기화\n","agent = Agent()\n","np.random.seed(0)\n","\n","# 3. 현재부터 max_step 까지 계산\n","max_step_number = 8\n","\n","# 4. 모든 상태에 대해\n","for max_step in range(max_step_number):\n","    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\n","    print(\"max_step = {}\".format(max_step))\n","    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n","    for i in range(env.reward.shape[0]):\n","        for j in range(env.reward.shape[1]):\n","            # 4.2 모든 행동에 대해\n","            for action in range(len(agent.action)):\n","                # 4.2.1 에이전트의 위치를 초기화\n","                agent.set_pos([i,j])\n","                # 4.2.2 현재 위치에서 행동 가치를 계산\n","                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n","\n","    q = np.round(q_table,2)\n","    print(\"Q - table\")\n","    show_q_table(q, env)\n","    print(\"High actions Arrow\")\n","    show_q_table_arrow(q,env)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTqhVC3ih5HW","executionInfo":{"status":"ok","timestamp":1649310298786,"user_tz":-540,"elapsed":4262,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}},"outputId":"89e9f5e4-22fa-4e2b-d926-252339c93d12"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["max_step = 0\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -0.75       |     -0.75       |     -0.75       |\n","| -0.75     -0.25 | -0.25     -0.25 | -0.25     -0.75 |\n","|     -0.25       |     -0.25       |     -0.25       |\n","+-----------------+-----------------+-----------------+\n","|     -0.25       |     -0.25       |     -0.25       |\n","| -0.75     -0.25 | -0.25     -0.25 | -0.25     -0.75 |\n","|     -0.25       |     -0.25       |      0.25       |\n","+-----------------+-----------------+-----------------+\n","|     -0.25       |     -0.25       |      1.00       |\n","| -0.75     -0.25 | -0.25      0.25 |  1.00      1.00 |\n","|     -0.75       |     -0.75       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |      ←  →     |      ←         |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|        ↑       |        ↑       |                 |\n","|          →     |      ←  →     |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|        ↑       |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n","max_step = 1\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -1.20       |     -1.09       |     -1.20       |\n","| -1.20     -0.59 | -0.70     -0.70 | -0.59     -1.20 |\n","|     -0.59       |     -0.48       |     -0.48       |\n","+-----------------+-----------------+-----------------+\n","|     -0.70       |     -0.59       |     -0.70       |\n","| -1.09     -0.48 | -0.59     -0.48 | -0.48     -0.98 |\n","|     -0.70       |     -0.48       |      1.15       |\n","+-----------------+-----------------+-----------------+\n","|     -0.59       |     -0.48       |      1.00       |\n","| -1.20     -0.48 | -0.70      1.15 |  1.00      1.00 |\n","|     -1.20       |     -0.98       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |                 |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |          →     |                 |\n","|                 |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n","max_step = 2\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -1.55       |     -1.42       |     -1.53       |\n","| -1.55     -0.92 | -1.05     -1.03 | -0.92     -1.53 |\n","|     -0.92       |     -0.73       |     -0.48       |\n","+-----------------+-----------------+-----------------+\n","|     -1.05       |     -0.92       |     -1.03       |\n","| -1.42     -0.73 | -0.92     -0.48 | -0.73     -0.98 |\n","|     -1.03       |     -0.48       |      1.15       |\n","+-----------------+-----------------+-----------------+\n","|     -0.92       |     -0.73       |      1.00       |\n","| -1.53     -0.48 | -1.03      1.15 |  1.00      1.00 |\n","|     -1.53       |     -0.98       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |                 |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |          →     |                 |\n","|                 |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n","max_step = 3\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -1.86       |     -1.70       |     -1.75       |\n","| -1.86     -1.20 | -1.36     -1.25 | -1.20     -1.75 |\n","|     -1.20       |     -0.88       |     -0.61       |\n","+-----------------+-----------------+-----------------+\n","|     -1.36       |     -1.20       |     -1.25       |\n","| -1.70     -0.88 | -1.20     -0.61 | -0.88     -1.11 |\n","|     -1.25       |     -0.61       |      1.15       |\n","+-----------------+-----------------+-----------------+\n","|     -1.20       |     -0.88       |      1.00       |\n","| -1.75     -0.61 | -1.25      1.15 |  1.00      1.00 |\n","|     -1.75       |     -1.11       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |                 |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |          →     |                 |\n","|                 |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n","max_step = 4\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -2.13       |     -1.92       |     -1.94       |\n","| -2.13     -1.42 | -1.63     -1.44 | -1.42     -1.94 |\n","|     -1.42       |     -1.06       |     -0.72       |\n","+-----------------+-----------------+-----------------+\n","|     -1.63       |     -1.42       |     -1.44       |\n","| -1.92     -1.06 | -1.42     -0.72 | -1.06     -1.22 |\n","|     -1.44       |     -0.72       |      1.15       |\n","+-----------------+-----------------+-----------------+\n","|     -1.42       |     -1.06       |      1.00       |\n","| -1.94     -0.72 | -1.44      1.15 |  1.00      1.00 |\n","|     -1.94       |     -1.22       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |                 |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |          →     |                 |\n","|                 |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n","max_step = 5\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -2.35       |     -2.11       |     -2.11       |\n","| -2.35     -1.61 | -1.85     -1.61 | -1.61     -2.11 |\n","|     -1.61       |     -1.21       |     -0.83       |\n","+-----------------+-----------------+-----------------+\n","|     -1.85       |     -1.61       |     -1.61       |\n","| -2.11     -1.21 | -1.61     -0.83 | -1.21     -1.33 |\n","|     -1.61       |     -0.83       |      1.15       |\n","+-----------------+-----------------+-----------------+\n","|     -1.61       |     -1.21       |      1.00       |\n","| -2.11     -0.83 | -1.61      1.15 |  1.00      1.00 |\n","|     -2.11       |     -1.33       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |                 |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |          →     |                 |\n","|                 |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n","max_step = 6\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -2.53       |     -2.27       |     -2.25       |\n","| -2.53     -1.77 | -2.03     -1.75 | -1.77     -2.25 |\n","|     -1.77       |     -1.35       |     -0.92       |\n","+-----------------+-----------------+-----------------+\n","|     -2.03       |     -1.77       |     -1.75       |\n","| -2.27     -1.35 | -1.77     -0.92 | -1.35     -1.42 |\n","|     -1.75       |     -0.92       |      1.15       |\n","+-----------------+-----------------+-----------------+\n","|     -1.77       |     -1.35       |      1.00       |\n","| -2.25     -0.92 | -1.75      1.15 |  1.00      1.00 |\n","|     -2.25       |     -1.42       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |                 |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |          →     |                 |\n","|                 |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n","max_step = 7\n","Q - table\n","+-----------------+-----------------+-----------------+\n","|     -2.69       |     -2.42       |     -2.37       |\n","| -2.69     -1.92 | -2.19     -1.87 | -1.92     -2.37 |\n","|     -1.92       |     -1.46       |     -1.01       |\n","+-----------------+-----------------+-----------------+\n","|     -2.19       |     -1.92       |     -1.87       |\n","| -2.42     -1.46 | -1.92     -1.01 | -1.46     -1.51 |\n","|     -1.87       |     -1.01       |      1.15       |\n","+-----------------+-----------------+-----------------+\n","|     -1.92       |     -1.46       |      1.00       |\n","| -2.37     -1.01 | -1.87      1.15 |  1.00      1.00 |\n","|     -2.37       |     -1.51       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","High actions Arrow\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |                 |                 |\n","|        ↓       |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|          →     |          →     |                 |\n","|                 |        ↓       |        ↓       |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |        ↑       |\n","|          →     |          →     |      ←  →     |\n","|                 |                 |        ↓       |\n","+-----------------+-----------------+-----------------+\n","\n"]}]}]}