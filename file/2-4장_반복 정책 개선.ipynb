{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-4장_반복 정책 개선.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNeMk9gRdI+vRRjTx/PsbWY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#반복 정책 개선"],"metadata":{"id":"gEJNiFMuxa6x"}},{"cell_type":"code","source":["import numpy as np\n","import time\n","import copy"],"metadata":{"id":"dL09IfYlxAz3","executionInfo":{"status":"ok","timestamp":1650091421764,"user_tz":-540,"elapsed":45,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["##그림 그리는 함수"],"metadata":{"id":"_QybMndKxW52"}},{"cell_type":"code","source":["# V table 그리기    \n","def show_v_table(v_table, env):    \n","    for i in range(env.reward.shape[0]):        \n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    print(\"                 |\",end=\"\")\n","                if k==1:\n","                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n","                if k==2:\n","                    print(\"                 |\",end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")\n","\n","# 정책 policy 화살표로 그리기\n","def show_policy(policy,env):\n","    for i in range(env.reward.shape[0]):        \n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    print(\"                 |\",end=\"\")\n","                if k==1:\n","                    if policy[i,j] == 0:\n","                        print(\"      ↑         |\",end=\"\")\n","                    elif policy[i,j] == 1:\n","                        print(\"      →         |\",end=\"\")\n","                    elif policy[i,j] == 2:\n","                        print(\"      ↓         |\",end=\"\")\n","                    elif policy[i,j] == 3:\n","                        print(\"      ←         |\",end=\"\")\n","                if k==2:\n","                    print(\"                 |\",end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")"],"metadata":{"id":"8kriUlj0xWee","executionInfo":{"status":"ok","timestamp":1650091449730,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##Environment 구현"],"metadata":{"id":"lgsQsky3w9nQ"}},{"cell_type":"code","source":["class Environment():\n","    \n","    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n","    cliff = -3\n","    road = -1\n","    goal = 1\n","    \n","    # 2. 목적지 좌표 설정\n","    goal_position = [2,2]\n","    \n","    # 3. 보상 리스트 숫자\n","    reward_list = [[road,road,road],\n","                   [road,road,road],\n","                   [road,road,goal]]\n","    \n","    # 4. 보상 리스트 문자\n","    reward_list1 = [[\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"goal\"]]\n","    \n","    # 5. 보상 리스트를 array로 설정\n","    def __init__(self):\n","        self.reward = np.asarray(self.reward_list)    \n","\n","    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n","    def move(self, agent, action):\n","        \n","        done = False\n","        \n","        # 6.1 행동에 따른 좌표 구하기\n","        new_pos = agent.pos + agent.action[action]\n","        \n","        # 6.2 현재좌표가 목적지 인지확인\n","        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n","            reward = self.goal\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.3 이동 후 좌표가 미로 밖인 확인    \n","        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n","            reward = self.cliff\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.4 이동 후 좌표가 길이라면\n","        else:\n","            observation = agent.set_pos(new_pos)\n","            reward = self.reward[observation[0],observation[1]]\n","            \n","        return observation, reward, done"],"metadata":{"id":"j-5_IGGDw6dI","executionInfo":{"status":"ok","timestamp":1650091455046,"user_tz":-540,"elapsed":359,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["##Agent 구현"],"metadata":{"id":"tT7seyk7xIBs"}},{"cell_type":"code","source":["class Agent():\n","    \n","    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n","    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n","    \n","    # 2. 각 행동별 선택확률\n","    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n","    \n","    # 3. 에이전트의 초기 위치 저장\n","    def __init__(self):\n","        self.pos = (0,0)\n","    \n","    # 4. 에이전트의 위치 저장\n","    def set_pos(self,position):\n","        self.pos = position\n","        return self.pos\n","    \n","    # 5. 에이전트의 위치 불러오기\n","    def get_pos(self):\n","        return self.pos"],"metadata":{"id":"XK7_RCYNxJyr","executionInfo":{"status":"ok","timestamp":1650091457656,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##정책 평가 및 개선 함수"],"metadata":{"id":"-RrVAIaox27F"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"OopZxTb4wSTK","executionInfo":{"status":"ok","timestamp":1650091462508,"user_tz":-540,"elapsed":354,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"outputs":[],"source":["def policy_evalution(env, agent, v_table, policy):\n","    gamma = 0.9\n","    while(True):\n","        # Δ←0\n","        delta = 0\n","        #  v←𝑉(𝑠)\n","        temp_v = copy.deepcopy(v_table)\n","        # 모든 𝑠∈𝑆에 대해 :\n","        for i in range(env.reward.shape[0]):\n","            for j in range(env.reward.shape[1]):\n","                # 에이전트를 지정된 좌표에 위치시킨후 가치함수를 계산\n","                agent.set_pos([i,j])\n","                # 현재 정책의 행동을 선택\n","                action = policy[i,j]\n","                observation, reward, done = env.move(agent, action)\n","                v_table[i,j] = reward + gamma * v_table[observation[0],observation[1]]\n","        # ∆←max⁡(∆,|v−𝑉(𝑠)|)\n","        # 계산전과 계산후의 가치의 차이를 계산\n","        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n","                \n","        # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n","        if delta < 0.000001:\n","            break\n","    return v_table, delta\n","\n","\n","def policy_improvement(env, agent, v_table, policy):\n","    \n","    # 67페이지 아래 누락 되어있습니다\n","    gamma = 0.9  \n","    \n","    # policyStable ← true \n","    policyStable = True\n","\n","    # 모든 s∈S에 대해：\n","    for i in range(env.reward.shape[0]):\n","        for j in range(env.reward.shape[1]):            \n","            # 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛←π(s) \n","            old_action = policy[i,j]            \n","            # 가능한 행동중 최댓값을 가지는 행동을 선택\n","            temp_action = 0\n","            temp_value =  -1e+10           \n","            for action in range(len(agent.action)):\n","                agent.set_pos([i,j])\n","                observation, reward, done = env.move(agent,action)\n","                if temp_value < reward + gamma * v_table[observation[0],observation[1]]:\n","                    temp_action = action\n","                    temp_value = reward + gamma * v_table[observation[0],observation[1]]\n","            # 만약 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛\"≠π(s)\"라면， \"policyStable ← False\" \n","            # old-action과 새로운 action이 다른지 체크\n","            if old_action != temp_action :\n","                policyStable = False\n","            policy[i,j] = temp_action\n","    return policy, policyStable\n","\n"]},{"cell_type":"code","source":["# 정책 반복\n","# 환경과 에이전트에 대한 초기 설정\n","np.random.seed(0)\n","env = Environment()\n","agent = Agent()\n","\n","# 1. 초기화\n","# 모든 𝑠∈𝑆에 대해 𝑉(𝑠)∈𝑅과 π(𝑠)∈𝐴(𝑠)를 임의로 설정\n","v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n","policy = np.random.randint(0, 4,(env.reward.shape[0], env.reward.shape[1]))\n","\n","print(\"Initial random V(S)\")\n","show_v_table(np.round(v_table,2),env)\n","print()\n","print(\"Initial random Policy π0(S)\")\n","show_policy(policy,env)\n","print(\"start policy iteration\")\n","\n","# 시작 시간을 변수에 저장\n","start_time = time.time()\n","\n","max_iter_number = 20000\n","for iter_number in range(max_iter_number):\n","    \n","    # 2.정책평가\n","    v_table, delta = policy_evalution(env, agent, v_table, policy)\n","\n","    # 정책 평가 후 결과 표시                                            \n","    print(\"\")\n","    print(\"Vπ{0:}(S) delta = {1:.10f}\".format(iter_number,delta))\n","    show_v_table(np.round(v_table,2),env)\n","    print()    \n","    \n","    \n","    # 3.정책개선\n","    policy, policyStable = policy_improvement(env, agent, v_table, policy)\n","\n","    # policy 변화 저장\n","    print(\"policy π{}(S)\".format(iter_number+1))\n","    show_policy(policy,env)\n","    # 하나라도 old-action과 새로운 action이 다르다면 '2. 정책평가'를 반복\n","    if(policyStable == True):\n","        break\n","\n","        \n","print(\"total_time = {}\".format(time.time()-start_time))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y90PiuNxwwWm","executionInfo":{"status":"ok","timestamp":1650091466516,"user_tz":-540,"elapsed":331,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}},"outputId":"f43dec87-6813-4b09-f471-99b27723d479"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial random V(S)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       0.55      |       0.72      |       0.60      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       0.54      |       0.42      |       0.65      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       0.44      |       0.89      |       0.96      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","\n","Initial random Policy π0(S)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      ↓         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      ←         |      ←         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      ↑         |      →         |      →         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","start policy iteration\n","\n","Vπ0(S) delta = 0.0000009713\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|     -28.00      |       6.20      |       8.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|     -30.00      |     -28.00      |      10.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|     -28.00      |      10.00      |      10.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","\n","policy π1(S)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      ↑         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↑         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","\n","Vπ1(S) delta = 0.0000002328\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       4.58      |       6.20      |       8.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       3.12      |       8.00      |      10.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       8.00      |      10.00      |      10.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","\n","policy π2(S)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↑         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","\n","Vπ2(S) delta = 0.0000001885\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       4.58      |       6.20      |       8.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       6.20      |       8.00      |      10.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       8.00      |      10.00      |      10.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","\n","policy π3(S)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↑         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","total_time = 0.03474235534667969\n"]}]}]}