{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-6장_몬테카를로 방법의 Prediction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8qAUJUDOB+oNg261fAlJs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#몬테카를로 방법의 Prediction"],"metadata":{"id":"gEJNiFMuxa6x"}},{"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm"],"metadata":{"id":"dL09IfYlxAz3","executionInfo":{"status":"ok","timestamp":1650536981680,"user_tz":-540,"elapsed":527,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["##그림 그리는 함수"],"metadata":{"id":"_QybMndKxW52"}},{"cell_type":"code","source":["# V table 그리기    \n","def show_v_table(v_table, env):    \n","    for i in range(env.reward.shape[0]):        \n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    print(\"                 |\",end=\"\")\n","                if k==1:\n","                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n","                if k==2:\n","                    print(\"                 |\",end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")"],"metadata":{"id":"8kriUlj0xWee","executionInfo":{"status":"ok","timestamp":1650536982224,"user_tz":-540,"elapsed":12,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["##Environment 구현"],"metadata":{"id":"lgsQsky3w9nQ"}},{"cell_type":"code","source":["class Environment():\n","    \n","    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n","    cliff = -3\n","    road = -1\n","    goal = 1\n","    \n","    # 2. 목적지 좌표 설정\n","    goal_position = [2,2]\n","    \n","    # 3. 보상 리스트 숫자\n","    reward_list = [[road,road,road],\n","                   [road,road,road],\n","                   [road,road,goal]]\n","    \n","    # 4. 보상 리스트 문자\n","    reward_list1 = [[\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"goal\"]]\n","    \n","    # 5. 보상 리스트를 array로 설정\n","    def __init__(self):\n","        self.reward = np.asarray(self.reward_list)    \n","\n","    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n","    def move(self, agent, action):\n","        \n","        done = False\n","        \n","        # 6.1 행동에 따른 좌표 구하기\n","        new_pos = agent.pos + agent.action[action]\n","        \n","        # 6.2 현재좌표가 목적지 인지확인\n","        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n","            reward = self.goal\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.3 이동 후 좌표가 미로 밖인 확인    \n","        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n","            reward = self.cliff\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.4 이동 후 좌표가 길이라면\n","        else:\n","            observation = agent.set_pos(new_pos)\n","            reward = self.reward[observation[0],observation[1]]\n","            \n","        return observation, reward, done"],"metadata":{"id":"j-5_IGGDw6dI","executionInfo":{"status":"ok","timestamp":1650536982225,"user_tz":-540,"elapsed":12,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["##Agent 구현"],"metadata":{"id":"tT7seyk7xIBs"}},{"cell_type":"code","source":["class Agent():\n","    \n","    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n","    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n","    \n","    # 2. 각 행동별 선택확률\n","    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n","    \n","    # 3. 에이전트의 초기 위치 저장\n","    def __init__(self):\n","        self.pos = (0,0)\n","    \n","    # 4. 에이전트의 위치 저장\n","    def set_pos(self,position):\n","        self.pos = position\n","        return self.pos\n","    \n","    # 5. 에이전트의 위치 불러오기\n","    def get_pos(self):\n","        return self.pos"],"metadata":{"id":"XK7_RCYNxJyr","executionInfo":{"status":"ok","timestamp":1650536982226,"user_tz":-540,"elapsed":12,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["##에피소드 생성 함수"],"metadata":{"id":"-RrVAIaox27F"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"OopZxTb4wSTK","executionInfo":{"status":"ok","timestamp":1650536982226,"user_tz":-540,"elapsed":11,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"outputs":[],"source":["def generate_episode(env, agent, first_visit):\n","    gamma = 0.09\n","    # 에피소드를 저장할 리스트\n","    episode = []\n","    # 이전에 방문여부 체크\n","    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n","    \n","    # 에이전트가 모든 상태에서 출발할 수 있게 출발지점을 무작위로 설정\n","    i = np.random.randint(0,env.reward.shape[0])\n","    j = np.random.randint(0,env.reward.shape[1])\n","    agent.set_pos([i,j])    \n","    #에피소드의 수익을 초기화\n","    G = 0\n","    #감쇄율의 지수\n","    step = 0\n","    max_step = 100\n","    # 에피소드 생성\n","    for k in range(max_step):\n","        pos = agent.get_pos()            \n","        action = np.random.randint(0,len(agent.action))            \n","        observaetion, reward, done = env.move(agent, action)    \n","        \n","        if first_visit:\n","            # 에피소드에 첫 방문한 상태인지 검사 :\n","            # visit[pos[0],pos[1]] == 0 : 첫 방문\n","            # visit[pos[0],pos[1]] == 1 : 중복 방문\n","            if visit[pos[0],pos[1]] == 0:   \n","                # 에피소드가 끝날때까지 G를 계산\n","                G += gamma**(step) * reward        \n","                # 방문 이력 표시\n","                visit[pos[0],pos[1]] = 1\n","                step += 1               \n","                # 방문 이력 저장(상태, 행동, 보상)\n","                episode.append((pos,action, reward))\n","        else:\n","            G += gamma**(step) * reward\n","            step += 1                   \n","            episode.append((pos,action,reward))            \n","\n","        # 에피소드가 종료했다면 루프에서 탈출\n","        if done == True:                \n","            break        \n","            \n","    return i, j, G, episode"]},{"cell_type":"markdown","source":["First-visit and Every-Visit MC Prediction"],"metadata":{"id":"gQtwLOM2qpLZ"}},{"cell_type":"code","source":["# first-visit MC and every-visit MC prediction\n","np.random.seed(0)\n","# 환경, 에이전트를 초기화\n","env = Environment()\n","agent = Agent()\n","\n","# 임의의 상태 가치 함수𝑉\n","v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n","\n","# 상태별로 에피소드 출발횟수를 저장하는 테이블\n","v_start = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n","\n","# 상태별로 도착지점 도착횟수를 저장하는 테이블\n","v_success = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n","\n","# 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해)\n","Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n","\n","# 최대 에피소드 수를 지정\n","max_episode = 100000\n","\n","# first visit 를 사용할지 every visit를 사용할 지 결정\n","# first_visit = True : first visit\n","# first_visit = False : every visit\n","first_visit = True\n","if first_visit:\n","    print(\"start first visit MC\")\n","else : \n","    print(\"start every visit MC\")\n","print()\n","\n","for epi in tqdm(range(max_episode)):\n","    \n","    i,j,G,episode = generate_episode(env, agent, first_visit)\n","    \n","    # 수익 𝐺를 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)에 추가(append)\n","    Return_s[i][j].append(G)\n","    \n","    # 에피소드 발생 횟수 계산\n","    episode_count = len(Return_s[i][j])\n","    # 상태별 발생한 수익의 총합 계산\n","    total_G = np.sum(Return_s[i][j])\n","    # 상태별 발생한 수익의 평균 계산\n","    v_table[i,j] = total_G / episode_count\n","    \n","  # 도착지점에 도착(reward = 1)했는지 체크    \n","    # episode[-1][2] : 에피소드 마지막 상태의 보상\n","    if episode[-1][2] == 1:\n","        v_success[i,j] += 1\n","\n","# 에피소드 출발 횟수 저장 \n","for i in range(env.reward.shape[0]):\n","    for j in range(env.reward.shape[1]):\n","        v_start[i,j] = len(Return_s[i][j])\n","        \n","print(\"V(s)\")\n","show_v_table(np.round(v_table,2),env)\n","print(\"V_start_count(s)\")\n","show_v_table(np.round(v_start,2),env)\n","print(\"V_success_pr(s)\")\n","show_v_table(np.round(v_success/v_start,2),env)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y90PiuNxwwWm","executionInfo":{"status":"ok","timestamp":1650537037760,"user_tz":-540,"elapsed":55545,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}},"outputId":"079351b5-1b02-4002-93f9-2df4cd6c2762"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["start first visit MC\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100000/100000 [00:55<00:00, 1803.68it/s]"]},{"output_type":"stream","name":"stdout","text":["V(s)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      -2.07      |      -1.61      |      -2.06      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      -1.62      |      -1.12      |      -1.06      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      -2.05      |      -1.05      |       1.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","V_start_count(s)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|   11302.00      |   11148.00      |   11021.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|   11077.00      |   11027.00      |   11109.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|   11151.00      |   11075.00      |   11090.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","V_success_pr(s)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       0.04      |       0.09      |       0.10      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       0.09      |       0.20      |       0.33      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|       0.10      |       0.33      |       1.00      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}