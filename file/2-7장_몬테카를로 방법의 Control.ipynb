{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-7장_몬테카를로 방법의 Control.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMlpB0rfF7kSywlqLSj+nY8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#몬테카를로 방법의 Control"],"metadata":{"id":"gEJNiFMuxa6x"}},{"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm"],"metadata":{"id":"dL09IfYlxAz3","executionInfo":{"status":"ok","timestamp":1651296571271,"user_tz":-540,"elapsed":359,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["##그림 그리는 함수"],"metadata":{"id":"_QybMndKxW52"}},{"cell_type":"code","source":["# Q table 그리기\n","def show_q_table(q_table,env):\n","    for i in range(env.reward.shape[0]):\n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n","                if k==1:\n","                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n","                if k==2:\n","                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")\n","\n","# 정책 policy 화살표로 그리기\n","def show_policy(policy,env):\n","    for i in range(env.reward.shape[0]):        \n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    print(\"                 |\",end=\"\")\n","                if k==1:\n","                    if policy[i,j] == 0:\n","                        print(\"      ↑         |\",end=\"\")\n","                    elif policy[i,j] == 1:\n","                        print(\"      →         |\",end=\"\")\n","                    elif policy[i,j] == 2:\n","                        print(\"      ↓         |\",end=\"\")\n","                    elif policy[i,j] == 3:\n","                        print(\"      ←         |\",end=\"\")\n","                if k==2:\n","                    print(\"                 |\",end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")"],"metadata":{"id":"8kriUlj0xWee","executionInfo":{"status":"ok","timestamp":1651296571969,"user_tz":-540,"elapsed":12,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["##Environment 구현"],"metadata":{"id":"lgsQsky3w9nQ"}},{"cell_type":"code","source":["class Environment():\n","    \n","    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n","    cliff = -3\n","    road = -1\n","    goal = 1\n","    \n","    # 2. 목적지 좌표 설정\n","    goal_position = [2,2]\n","    \n","    # 3. 보상 리스트 숫자\n","    reward_list = [[road,road,road],\n","                   [road,road,road],\n","                   [road,road,goal]]\n","    \n","    # 4. 보상 리스트 문자\n","    reward_list1 = [[\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"goal\"]]\n","    \n","    # 5. 보상 리스트를 array로 설정\n","    def __init__(self):\n","        self.reward = np.asarray(self.reward_list)    \n","\n","    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n","    def move(self, agent, action):\n","        \n","        done = False\n","        \n","        # 6.1 행동에 따른 좌표 구하기\n","        new_pos = agent.pos + agent.action[action]\n","        \n","        # 6.2 현재좌표가 목적지 인지확인\n","        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n","            reward = self.goal\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.3 이동 후 좌표가 미로 밖인 확인    \n","        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n","            reward = self.cliff\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.4 이동 후 좌표가 길이라면\n","        else:\n","            observation = agent.set_pos(new_pos)\n","            reward = self.reward[observation[0],observation[1]]\n","            \n","        return observation, reward, done"],"metadata":{"id":"j-5_IGGDw6dI","executionInfo":{"status":"ok","timestamp":1651296571970,"user_tz":-540,"elapsed":12,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["##Agent 구현"],"metadata":{"id":"tT7seyk7xIBs"}},{"cell_type":"code","source":["class Agent():\n","    \n","    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n","    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n","    \n","    # 2. 각 행동별 선택확률\n","    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n","    \n","    # 3. 에이전트의 초기 위치 저장\n","    def __init__(self):\n","        self.pos = (0,0)\n","    \n","    # 4. 에이전트의 위치 저장\n","    def set_pos(self,position):\n","        self.pos = position\n","        return self.pos\n","    \n","    # 5. 에이전트의 위치 불러오기\n","    def get_pos(self):\n","        return self.pos"],"metadata":{"id":"XK7_RCYNxJyr","executionInfo":{"status":"ok","timestamp":1651296571970,"user_tz":-540,"elapsed":11,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["##에피소드 생성 함수"],"metadata":{"id":"-RrVAIaox27F"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"OopZxTb4wSTK","executionInfo":{"status":"ok","timestamp":1651296571971,"user_tz":-540,"elapsed":11,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"outputs":[],"source":["def generate_episode_with_policy(env, agent, first_visit, policy):\n","    gamma = 0.09\n","    # 에피소드를 저장할 리스트\n","    episode = []\n","    # 이전에 방문여부 체크\n","    visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n","    \n","    # 에이전트는 항상 (0,0)에서 출발\n","    i = 0\n","    j = 0\n","    agent.set_pos([i,j])    \n","    #에피소드의 수익을 초기화\n","    G = 0\n","    #감쇄율의 지수\n","    step = 0\n","    max_step = 100\n","    # 에피소드 생성\n","    for k in range(max_step):\n","        pos = agent.get_pos()        \n","        # 현재 상태의 정책을 이용해 행동을 선택한 후 이동\n","        action = np.random.choice(range(0,len(agent.action)), p=policy[pos[0],pos[1],:]) \n","        observaetion, reward, done = env.move(agent, action)    \n","        \n","        if first_visit:\n","            # 에피소드에 첫 방문한 상태인지 검사 :\n","            # visit[pos[0],pos[1]] == 0 : 첫 방문\n","            # visit[pos[0],pos[1]] == 1 : 중복 방문\n","            if visit[pos[0],pos[1],action] == 0:   \n","                # 에피소드가 끝날때까지 G를 계산\n","                G += gamma**(step) * reward        \n","                # 방문 이력 표시\n","                visit[pos[0],pos[1],action] = 1\n","                step += 1               \n","                # 방문 이력 저장(상태, 행동, 보상)\n","                episode.append((pos,action, reward))\n","        else:\n","            G += gamma**(step) * reward\n","            step += 1                   \n","            episode.append((pos,action,reward))            \n","\n","        # 에피소드가 종료했다면 루프에서 탈출\n","        if done == True:                \n","            break        \n","            \n","    return i, j, G, episode"]},{"cell_type":"markdown","source":["ϵ-정책을 이용하는 몬테카를로 control 알고리즘"],"metadata":{"id":"gQtwLOM2qpLZ"}},{"cell_type":"code","source":["np.random.seed(0)\n","# 환경, 에이전트를 초기화\n","env = Environment()\n","agent = Agent()\n","\n","# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n","# # 𝑄(𝑠,𝑎)←임의의 값 (행동 개수, 미로 세로, 미로 가로)\n","Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n","print(\"Initial Q(s,a)\")\n","show_q_table(Q_table,env)\n","\n","# 상태를 방문한 횟수를 저장하는 테이블\n","Q_visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n","\n","# 미로 모든 상태에서 최적 행동을 저장하는 테이블\n","# 각 상태에서 Q 값이 가장 큰 행동을 선택 후 optimal_a 에 저장\n","optimal_a = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n","for i in range(env.reward.shape[0]):\n","    for j in range(env.reward.shape[1]):\n","        optimal_a[i,j] = np.argmax(Q_table[i,j,:])\n","print(\"initial optimal_a\")\n","show_policy(optimal_a,env)\n","\n","# π(𝑠,𝑎)←임의의 𝜖−탐욕 정책\n","# 무작위로 행동을 선택하도록 지정\n","policy = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n","\n","# 한 상태에서 가능한 확률의 합이 1이 되도록 계산\n","epsilon = 0.8\n","for i in range(env.reward.shape[0]):\n","    for j in range(env.reward.shape[1]):\n","        for k in range(len(agent.action)):\n","            if optimal_a[i,j] == k:\n","                policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n","            else:\n","                policy[i,j,k] = epsilon/len(agent.action)\n","print(\"Initial Policy\")\n","show_q_table(policy,env)\n","\n","\n","# 최대 에피소드 수 길이를 지정\n","max_episode = 10000\n","\n","# first visit 를 사용할지 every visit를 사용할 지 결정\n","# first_visit = True : first visit\n","# first_visit = False : every visit\n","first_visit = True\n","if first_visit:\n","    print(\"start first visit MC\")\n","else : \n","    print(\"start every visit MC\")\n","print()\n","\n","gamma = 0.09\n","for epi in tqdm(range(max_episode)):\n","# for epi in range(max_episode):\n","\n","    # π를 이용해서 에피소드 1개를 생성\n","    x,y,G,episode = generate_episode_with_policy(env, agent, first_visit, policy)\n","    \n","    for step_num in range(len(episode)):\n","        G = 0\n","        # episode[step_num][0][0] : step_num번째 방문한 상태의 x 좌표\n","        # episode[step_num][0][1] : step_num번째 방문한 상태의 y 좌표\n","        # episode[step_num][1] : step_num번째 상태에서 선택한 행동\n","        i = episode[step_num][0][0]\n","        j = episode[step_num][0][1]\n","        action = episode[step_num][1]\n","        \n","        # 에피소드 시작점을 카운트\n","        Q_visit[i,j,action] += 1\n","\n","        # 서브 에피소드 (episode[step_num:])의 출발부터 끝까지 수익 G를 계산\n","        # k[2] : episode[step_num][2] 과 같으며 step_num 번째 받은 보상\n","        # step : 감쇄율\n","        for step, k in enumerate(episode[step_num:]):\n","            G += gamma**(step)*k[2]\n","\n","        # Incremental mean : 𝑄(𝑠,𝑎)←𝑎𝑣𝑒𝑟𝑎𝑔𝑒(𝑅𝑒𝑡𝑢𝑟𝑛(𝑠,𝑎)) \n","        Q_table[i,j,action] += 1 / Q_visit[i,j,action]*(G-Q_table[i,j,action])\n","    \n","    # (c) 에피소드 안의 각 s에 대해서 :\n","    # 미로 모든 상태에서 최적 행동을 저장할 공간 마련\n","    # 𝑎∗ ←argmax_a 𝑄(𝑠,𝑎)\n","    for i in range(env.reward.shape[0]):\n","        for j in range(env.reward.shape[1]):\n","            optimal_a[i,j] = np.argmax(Q_table[i,j,:])            \n","   \n","    # 모든 𝑎∈𝐴(𝑆) 에 대해서 :\n","    # 새로 계산된 optimal_a 를 이용해서 행동 선택 확률 policy (π) 갱신\n","    epsilon = 1 - epi/max_episode\n","\n","    for i in range(env.reward.shape[0]):\n","        for j in range(env.reward.shape[1]):\n","            for k in range(len(agent.action)):\n","                if optimal_a[i,j] == k:\n","                    policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n","                else:\n","                    policy[i,j,k] = epsilon/len(agent.action)\n","\n","print(\"Final Q(s,a)\")\n","show_q_table(Q_table,env)\n","print(\"Final policy\")\n","show_q_table(policy,env)\n","print(\"Final optimal_a\")\n","show_policy(optimal_a,env)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y90PiuNxwwWm","executionInfo":{"status":"ok","timestamp":1651296575122,"user_tz":-540,"elapsed":3162,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}},"outputId":"ed7d6d9f-4a10-4db4-8d0f-ed4f7dee3f8e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Q(s,a)\n","+-----------------+-----------------+-----------------+\n","|      0.55       |      0.42       |      0.96       |\n","|  0.54      0.72 |  0.89      0.65 |  0.53      0.38 |\n","|      0.60       |      0.44       |      0.79       |\n","+-----------------+-----------------+-----------------+\n","|      0.57       |      0.02       |      0.98       |\n","|  0.09      0.93 |  0.87      0.83 |  0.78      0.80 |\n","|      0.07       |      0.78       |      0.46       |\n","+-----------------+-----------------+-----------------+\n","|      0.12       |      0.52       |      0.46       |\n","|  0.94      0.64 |  0.77      0.41 |  0.62      0.57 |\n","|      0.14       |      0.26       |      0.02       |\n","+-----------------+-----------------+-----------------+\n","initial optimal_a\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      ←         |      ↑         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      ←         |      ↑         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      ←         |      ←         |      ←         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","Initial Policy\n","+-----------------+-----------------+-----------------+\n","|      0.20       |      0.20       |      0.40       |\n","|  0.20      0.40 |  0.40      0.20 |  0.20      0.20 |\n","|      0.20       |      0.20       |      0.20       |\n","+-----------------+-----------------+-----------------+\n","|      0.20       |      0.20       |      0.40       |\n","|  0.20      0.40 |  0.40      0.20 |  0.20      0.20 |\n","|      0.20       |      0.20       |      0.20       |\n","+-----------------+-----------------+-----------------+\n","|      0.20       |      0.20       |      0.20       |\n","|  0.40      0.20 |  0.40      0.20 |  0.40      0.20 |\n","|      0.20       |      0.20       |      0.20       |\n","+-----------------+-----------------+-----------------+\n","start first visit MC\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:03<00:00, 3270.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Final Q(s,a)\n","+-----------------+-----------------+-----------------+\n","|     -3.00       |     -3.00       |     -3.00       |\n","| -3.00     -1.13 | -1.17     -1.16 | -1.13     -3.00 |\n","|     -1.12       |     -1.09       |     -1.02       |\n","+-----------------+-----------------+-----------------+\n","|     -1.17       |     -1.13       |     -1.14       |\n","| -3.00     -1.09 | -1.12     -0.96 | -1.03     -3.00 |\n","|     -1.15       |     -1.02       |      1.09       |\n","+-----------------+-----------------+-----------------+\n","|     -1.14       |     -1.08       |      1.00       |\n","| -3.00     -1.01 | -1.14      1.09 |  1.00      1.00 |\n","|     -3.00       |     -3.00       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","Final policy\n","+-----------------+-----------------+-----------------+\n","|      0.00       |      0.00       |      0.00       |\n","|  0.00      0.00 |  0.00      0.00 |  0.00      0.00 |\n","|      1.00       |      1.00       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","|      0.00       |      0.00       |      0.00       |\n","|  0.00      1.00 |  0.00      1.00 |  0.00      0.00 |\n","|      0.00       |      0.00       |      1.00       |\n","+-----------------+-----------------+-----------------+\n","|      0.00       |      0.00       |      1.00       |\n","|  0.00      1.00 |  0.00      1.00 |  0.00      0.00 |\n","|      0.00       |      0.00       |      0.00       |\n","+-----------------+-----------------+-----------------+\n","Final optimal_a\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      ↓         |      ↓         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↓         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      →         |      →         |      ↑         |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}