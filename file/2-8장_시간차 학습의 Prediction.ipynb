{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-8장_시간차 학습의 Prediction.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOffGB1nPEvqXm1EJLmqC24"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#시간차 학습의 Prediction"],"metadata":{"id":"5agojReWIYlV"}},{"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm"],"metadata":{"id":"CwtfaBKOhqaB","executionInfo":{"status":"ok","timestamp":1651652285485,"user_tz":-540,"elapsed":337,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["##그림그리는 함수"],"metadata":{"id":"JQkVjrbvI2_s"}},{"cell_type":"code","source":["# V table 그리기    \n","def show_v_table(v_table, env):    \n","    for i in range(env.reward.shape[0]):        \n","        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","        print(\"+\")\n","        for k in range(3):\n","            print(\"|\",end=\"\")\n","            for j in range(env.reward.shape[1]):\n","                if k==0:\n","                    print(\"                 |\",end=\"\")\n","                if k==1:\n","                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n","                if k==2:\n","                    print(\"                 |\",end=\"\")\n","            print()\n","    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n","    print(\"+\")"],"metadata":{"id":"-maf_uhdI2k9","executionInfo":{"status":"ok","timestamp":1651652285825,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["##Environment 구현"],"metadata":{"id":"8EIdUJwmIgFS"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"HytMoTUaeV4-","executionInfo":{"status":"ok","timestamp":1651652285825,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"outputs":[],"source":["class Environment():\n","    \n","    # 보상 설정\n","    cliff = -3\n","    road = -1\n","    goal = 1\n","\n","    reward_list = [[road,road,road],\n","                   [road,road,road],\n","                   [road,road,goal]]\n","\n","    reward_list1 = [[\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"road\"],\n","                    [\"road\",\"road\",\"goal\"]]\n","\n","    def __init__(self):\n","        self.reward = np.array(self.reward_list)\n","     \n","    def move(self, agent, action):\n","        \n","        done = False\n","\n","        new_pos = agent.pos + agent.action[action]\n","\n","         # 6.2 현재좌표가 목적지 인지확인\n","        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n","            reward = self.goal\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.3 이동 후 좌표가 미로 밖인 확인    \n","        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n","            reward = self.cliff\n","            observation = agent.set_pos(agent.pos)\n","            done = True\n","        # 6.4 이동 후 좌표가 길이라면\n","        else:\n","            observation = agent.set_pos(new_pos)\n","            reward = self.reward[observation[0],observation[1]]\n","            \n","        return observation, reward, done\n","      "]},{"cell_type":"markdown","source":["##Agent 구현"],"metadata":{"id":"eNm5b3QNIhjx"}},{"cell_type":"code","source":["class Agent():\n","\n","    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n","    \n","    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n","\n","    def __init__(self):\n","        self.pos = (0,0)\n","    \n","    def set_pos(self,position):\n","        self.pos = position\n","        return self.pos\n","\n","    def get_pos(self):\n","        return self.pos"],"metadata":{"id":"KApr6k3LiJmF","executionInfo":{"status":"ok","timestamp":1651652286252,"user_tz":-540,"elapsed":9,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# TD(0) prediction\n","np.random.seed(0)\n","# 환경, 에이전트를 초기화\n","env = Environment()\n","agent = Agent()\n","gamma = 0.9\n","\n","#초기화 : \n","#π← 평가할 정책\n","# 가능한 모든 행동이 무작위로 선택되도록 지정\n","#𝑉← 임의의 상태가치 함수\n","V = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n","\n","# 최대 에피소드, 에피소드의 최대 길이를 지정\n","max_episode = 10000\n","max_step = 100\n","\n","alpha = 0.01\n","\n","print(\"start TD(0) prediction\")\n","\n","# 각 에피소드에 대해 반복 :\n","for epi in tqdm(range(max_episode)):\n","    delta =0\n","    # s 를 초기화\n","    i = 0\n","    j = 0\n","    agent.set_pos([i,j])\n","\n","    #  에피소드의 각 스텝에 대해 반복 :\n","    for k in range(max_step):\n","        pos = agent.get_pos()\n","        # a←상태 𝑠 에서 정책 π에 의해 결정된 행동 \n","        # 가능한 모든 행동이 무작위로 선택되게 함\n","        action = np.random.randint(0,4)\n","        # 행동 a 를 취한 후 보수 r과 다음 상태 s’를 관측\n","        # s←𝑠'\n","        observation, reward, done = env.move(agent,action)\n","        # V(𝑠)←V(𝑠)+ α[𝑟+𝛾𝑉(𝑠^)−𝑉(𝑠)]\n","        V[pos[0],pos[1]] += alpha * (reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]])\n","        # s가 마지막 상태라면 종료\n","        if done == True:\n","            break\n","            \n","print(\"V(s)\")\n","show_v_table(np.round(V,2),env)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBxFRf1JIUHU","executionInfo":{"status":"ok","timestamp":1651652286610,"user_tz":-540,"elapsed":366,"user":{"displayName":"Jungi Kim","userId":"13599710065611566056"}},"outputId":"90b10be8-a688-45a3-b203-febac49c0680"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["start TD(0) prediction\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:00<00:00, 24941.27it/s]"]},{"output_type":"stream","name":"stdout","text":["V(s)\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|     -12.59      |     -11.01      |     -10.12      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|     -10.85      |      -8.53      |      -5.80      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n","|                 |                 |                 |\n","|      -9.68      |      -6.06      |       3.48      |\n","|                 |                 |                 |\n","+-----------------+-----------------+-----------------+\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}